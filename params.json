{"name":"PracticalML","tagline":"Practical Machine Learning - Coursera John Hopkins","body":"# Goal\r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.\r\n\r\nIn this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict how well the subjects are doing the exercises.\r\n\r\n# Data\r\n\r\nThe subjects were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).\r\n\r\nThe training data for this project are available here: \r\n\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\r\n\r\nThe test data are available here: \r\n\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\r\n\r\nThe correct outcomes are in the \"classe\" variable in the training set.\r\n\r\n# Approach\r\n\r\n1. Get train and test data\r\n2. Divide train data into train/validate\r\n3. Clean up the train data\r\n4. Build model\r\n5. Estimate out of sample error\r\n6. Apply on test data to predict outcomes\r\n\r\n## Data cleaning\r\n\r\nUsing exploratory analysis it was clear that many columns in the training set had missing data. Using simple\r\nvisualization and tabularization we have de-selected the columns that had a large percentage of missing values. \r\nWe have been considering techniques to fill in the missings using KNN or such but decided to only do so if the\r\nexpected out of sample error was large. This turned out not to be the case, so the model was built with this\r\nsmaller subset of columns.\r\n\r\nA second problem was in the format of the data itself. Some values in the trainingset looked like the Excel\r\ndivide-by-zero error and we replaced those by NA. Also, there was one row which seemed incorrectly formatted:\r\nrow 15350 seemed to have an comma extra, resulting in a mismatch of columns. This was manually fixed.\r\n\r\n## Model creation\r\n\r\nAs the outcome was a categorial variable, the obvious choice was to use some sort of tree building model. We\r\nstarted with simple CART but the out of sample error was big (only ca 50% correct). Switching to the bagged\r\nversion of it (method=\"treebag\") made a tremendous improvement.\r\n\r\n## Out of Sample error\r\n\r\nThe results in a table of predicted outcome on the validation set (40% of the sample) vs the actuals from the\r\nvalidation set are as follows:\r\n\r\n```\r\npreds         A    B    C    D    E\r\n         0    0    0    0    0    0\r\n    A    0 2230   11    0    0    0\r\n    B    0    1 1501    8    3    1\r\n    C    0    0    5 1358   10    0\r\n    D    0    0    1    2 1272    4\r\n    E    0    0    0    0    1 1437\r\n```\r\n\r\nThe percentage correct predictions is **99.40%**.\r\n\r\nThe expected **out of sample error** is thus **0.60%**.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}